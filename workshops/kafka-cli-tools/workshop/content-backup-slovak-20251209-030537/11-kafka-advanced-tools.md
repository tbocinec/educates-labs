---
title: Advanced CLI Tools
---

# üéØ Advanced Kafka CLI Tools

V tejto lekcii sa nauƒç√≠≈° pou≈æ√≠va≈• pokroƒçil√© Kafka CLI n√°stroje pre deep debugging a maintenance.

## N√°stroje v tejto lekcii

- ‚úÖ **kafka-dump-log.sh** - Raw log inspection
- ‚úÖ **kafka-get-offsets.sh** - Offset queries
- ‚úÖ **kafka-delete-records.sh** - Manual cleanup
- ‚úÖ **kafka-broker-api-versions.sh** - API compatibility

**Kedy pou≈æi≈•:**
- Deep debugging (corruption, offset issues)
- GDPR compliance (delete specific records)
- Capacity planning (offset analysis)
- Upgrade planning (API compatibility)

---

## 1Ô∏è‚É£ kafka-dump-log.sh

### ƒåo je dump-log?

Umo≈æ≈àuje ƒç√≠ta≈• **raw log segments** priamo z disku.

**Use cases:**
- Inspect log file structure
- Debug corruption
- Analyze offset gaps
- Low-level troubleshooting

---

### Help

```terminal:execute
command: docker exec kafka-1 kafka-dump-log.sh --help
```

**D√¥le≈æit√© parametre:**
- `--files` - Cesta k log file (povinn√©)
- `--print-data-log` - Vyp√≠≈° messages
- `--deep-iteration` - Deep scan (slower, thorough)
- `--offsets-decoder` - Decode offset metadata

---

### Find Log Files

Log files s√∫ v `/var/lib/kafka/data`:

**Vytvor√≠me t√©mu pre dump demo:**
```terminal:execute
command: docker exec kafka-1 kafka-topics.sh --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 --create --topic dump-demo --partitions 2 --replication-factor 2 --if-not-exists
```

**Po≈°leme messages:**
```terminal:execute
command: |
  for i in {1..20}; do
    echo "Dump test message $i"
  done | docker exec -i kafka-1 kafka-console-producer.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic dump-demo
```

**Find log directory:**
```terminal:execute
command: |
  docker exec kafka-1 find /var/lib/kafka/data -name "dump-demo-*" -type d
```

Output:
```
/var/lib/kafka/data/dump-demo-0
/var/lib/kafka/data/dump-demo-1
```

**List log files:**
```terminal:execute
command: |
  docker exec kafka-1 ls -lh /var/lib/kafka/data/dump-demo-0/
```

Output:
```
00000000000000000000.log      ‚Üê Active log segment
00000000000000000000.index    ‚Üê Offset index
00000000000000000000.timeindex ‚Üê Timestamp index
```

---

### Dump Log Metadata

**Dump log file metadata (NO messages):**
```terminal:execute
command: |
  docker exec kafka-1 kafka-dump-log.sh \
    --files /var/lib/kafka/data/dump-demo-0/00000000000000000000.log | head -30
```

Output:
```
Dumping /var/lib/kafka/data/dump-demo-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 9 count: 10 baseSequence: 0 ...
```

---

### Dump with Messages

**Dump s messages (print data):**
```terminal:execute
command: |
  docker exec kafka-1 kafka-dump-log.sh \
    --files /var/lib/kafka/data/dump-demo-0/00000000000000000000.log \
    --print-data-log | head -50
```

Output:
```
offset: 0 position: 0 CreateTime: 1702124000000 size: 25 magic: 2 payload: Dump test message 1
offset: 1 position: 25 CreateTime: 1702124001000 size: 25 magic: 2 payload: Dump test message 2
...
```

**D√¥le≈æit√© fields:**
- `offset` - Logical offset
- `position` - Physical position v file
- `CreateTime` - Message timestamp
- `size` - Message size v bytes
- `payload` - Actual message

---

### Deep Iteration

**Thorough scan (slower, detects corruption):**
```terminal:execute
command: |
  docker exec kafka-1 kafka-dump-log.sh \
    --files /var/lib/kafka/data/dump-demo-0/00000000000000000000.log \
    --deep-iteration \
    --print-data-log | tail -20
```

üí° **Deep iteration overuje checksumy - detects corruption!**

---

## 2Ô∏è‚É£ kafka-get-offsets.sh

### ƒåo je get-offsets?

Zis≈•uje offsety pre t√©my (earliest, latest, timestamp-based).

**Use cases:**
- Capacity calculation (koƒæko messages?)
- Timestamp-based offset lookup
- Monitoring offset growth

---

### Help

```terminal:execute
command: docker exec kafka-1 kafka-run-class.sh kafka.tools.GetOffsetShell --help
```

**D√¥le≈æit√© parametre:**
- `--bootstrap-server` - Kafka broker address
- `--topic` - Topic name
- `--time` - Timestamp (-1 = latest, -2 = earliest)

---

### Get Latest Offsets

**Latest offset pre dump-demo:**
```terminal:execute
command: |
  docker exec kafka-1 kafka-run-class.sh kafka.tools.GetOffsetShell \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic dump-demo \
    --time -1
```

Output:
```
dump-demo:0:10
dump-demo:1:10
```

Format: `topic:partition:offset`

---

### Get Earliest Offsets

**Earliest offset:**
```terminal:execute
command: |
  docker exec kafka-1 kafka-run-class.sh kafka.tools.GetOffsetShell \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic dump-demo \
    --time -2
```

Output:
```
dump-demo:0:0
dump-demo:1:0
```

---

### Calculate Total Messages

**Total messages v t√©me:**
```terminal:execute
command: |
  echo "Calculating total messages in dump-demo..."
  docker exec kafka-1 kafka-run-class.sh kafka.tools.GetOffsetShell \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic dump-demo \
    --time -1 | awk -F: '{sum += $3} END {print "Total messages:", sum}'
```

---

### Timestamp-Based Offset

**Find offset for specific timestamp:**

```terminal:execute
command: |
  # Current timestamp - 5 minutes
  TIMESTAMP=$(($(date +%s) * 1000 - 300000))
  echo "Looking for offset at timestamp: $TIMESTAMP"
  
  docker exec kafka-1 kafka-run-class.sh kafka.tools.GetOffsetShell \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic dump-demo \
    --time $TIMESTAMP
```

üí° **Timestamp v milliseconds!**

---

## 3Ô∏è‚É£ kafka-delete-records.sh

### ƒåo je delete-records?

Vyma≈æe records **pred konkr√©tnym offsetom** (permanent!).

**Use cases:**
- GDPR compliance (delete user data)
- Free up space quickly
- Remove corrupted messages
- Manual cleanup pred retention

‚ö†Ô∏è **PERMANENT DELETION - use with caution!**

---

### Help

```terminal:execute
command: docker exec kafka-1 kafka-delete-records.sh --help
```

**D√¥le≈æit√© parametre:**
- `--bootstrap-server` - Kafka broker address
- `--offset-json-file` - JSON file s offsetmi

---

### Delete Records Example

**Vytvor√≠me t√©mu pre delete demo:**
```terminal:execute
command: docker exec kafka-1 kafka-topics.sh --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 --create --topic delete-demo --partitions 2 --replication-factor 2 --if-not-exists
```

**Po≈°leme 50 messages:**
```terminal:execute
command: |
  for i in {1..50}; do
    echo "Delete demo message $i"
  done | docker exec -i kafka-1 kafka-console-producer.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic delete-demo
```

**Check messages (should be 50):**
```terminal:execute
command: |
  docker exec kafka-1 kafka-run-class.sh kafka.tools.GetOffsetShell \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic delete-demo \
    --time -1 | awk -F: '{sum += $3} END {print "Messages before delete:", sum}'
```

---

### Create Delete JSON

**Vytvor√≠me JSON file - delete prv√Ωch 20 messages z ka≈ædej part√≠cie:**
```terminal:execute
command: |
  docker exec kafka-1 sh -c 'cat > /tmp/delete-offsets.json <<EOF
{
  "partitions": [
    {
      "topic": "delete-demo",
      "partition": 0,
      "offset": 20
    },
    {
      "topic": "delete-demo",
      "partition": 1,
      "offset": 20
    }
  ],
  "version": 1
}
EOF'
```

**Verify JSON:**
```terminal:execute
command: docker exec kafka-1 cat /tmp/delete-offsets.json
```

---

### Execute Delete

**DELETE RECORDS (permanent!):**
```terminal:execute
command: |
  docker exec kafka-1 kafka-delete-records.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --offset-json-file /tmp/delete-offsets.json
```

Output:
```
Deleting records for partition delete-demo-0 with offset 20
Deleting records for partition delete-demo-1 with offset 20
Records deleted successfully.
```

---

### Verify Deletion

**Check earliest offset (should be 20 now):**
```terminal:execute
command: |
  docker exec kafka-1 kafka-run-class.sh kafka.tools.GetOffsetShell \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic delete-demo \
    --time -2
```

Output:
```
delete-demo:0:20  ‚Üê Was 0, now 20!
delete-demo:1:20  ‚Üê Was 0, now 20!
```

**Try to read deleted messages (should fail):**
```terminal:execute
command: |
  docker exec kafka-1 kafka-console-consumer.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic delete-demo \
    --partition 0 \
    --offset 0 \
    --max-messages 1 \
    --timeout-ms 3000 || echo "Cannot read offset 0 - deleted!"
```

**Read from offset 20 (works):**
```terminal:execute
command: |
  docker exec kafka-1 kafka-console-consumer.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic delete-demo \
    --partition 0 \
    --offset 20 \
    --max-messages 5 \
    --timeout-ms 3000
```

üíÄ **Messages 0-19 s√∫ PERMANENTLY deleted!**

---

## 4Ô∏è‚É£ kafka-broker-api-versions.sh

### ƒåo je broker-api-versions?

Zis≈•uje **API versions** podporovan√© brokerom.

**Use cases:**
- Upgrade planning (client compatibility)
- Debugging version mismatches
- Feature detection

---

### Check API Versions

**Query broker API versions:**
```terminal:execute
command: |
  docker exec kafka-1 kafka-broker-api-versions.sh \
    --bootstrap-server kafka-1:9092 | head -30
```

Output:
```
kafka-1:9092 (id: 1 rack: null) -> (
  Produce(0): 0 to 9 [usable: 9],
  Fetch(1): 0 to 13 [usable: 13],
  ListOffsets(2): 0 to 7 [usable: 7],
  Metadata(3): 0 to 12 [usable: 12],
  ...
)
```

**D√¥le≈æit√©:**
- `usable: 9` = broker podporuje API version 9
- Client mus√≠ pou≈æ√≠va≈• version <= 9

---

### Specific API Check

**Find Produce API versions:**
```terminal:execute
command: |
  docker exec kafka-1 kafka-broker-api-versions.sh \
    --bootstrap-server kafka-1:9092 | grep "Produce"
```

**Find Consumer API versions:**
```terminal:execute
command: |
  docker exec kafka-1 kafka-broker-api-versions.sh \
    --bootstrap-server kafka-1:9092 | grep "Fetch"
```

---

## üéØ Use Cases

### 1. GDPR - Delete User Data
**Scenario:** User requested data deletion:

```terminal:execute
command: |
  # Create GDPR demo topic
  docker exec kafka-1 kafka-topics.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --create --topic user-events --partitions 1 --replication-factor 2 --if-not-exists
```

```terminal:execute
command: |
  # Send user events
  docker exec -i kafka-1 kafka-console-producer.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic user-events \
    --property "parse.key=true" \
    --property "key.separator=:" <<EOF
user123:signup_event
user456:login_event
user123:purchase_event
user789:page_view
user123:logout_event
EOF
```

```terminal:execute
command: |
  # User123 requested deletion
  # Find offset range for user123 (manual inspection)
  docker exec kafka-1 kafka-console-consumer.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic user-events \
    --from-beginning \
    --property print.key=true \
    --property print.offset=true \
    --timeout-ms 3000
```

```terminal:execute
command: |
  # Delete records up to offset 5 (includes user123 data)
  docker exec kafka-1 sh -c 'cat > /tmp/gdpr-delete.json <<EOF
{
  "partitions": [{"topic": "user-events", "partition": 0, "offset": 5}],
  "version": 1
}
EOF'
  
  docker exec kafka-1 kafka-delete-records.sh \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --offset-json-file /tmp/gdpr-delete.json
```

üí° **GDPR compliance achieved!**

---

### 2. Debug Corrupted Segment
**Scenario:** Consumer reports deserialization errors:

```terminal:execute
command: |
  # Inspect raw log file
  docker exec kafka-1 sh -c '
    LOG_FILE=$(find /var/lib/kafka/data/dump-demo-0 -name "*.log" | head -1)
    kafka-dump-log.sh --files $LOG_FILE --deep-iteration --print-data-log | tail -20
  '
```

---

### 3. Capacity Planning
**Scenario:** Pl√°nujeme retention policy:

```terminal:execute
command: |
  echo "=== Capacity Analysis ==="
  
  # Messages per day (example)
  MESSAGES_PER_DAY=1000000
  
  # Current total messages
  CURRENT=$(docker exec kafka-1 kafka-run-class.sh kafka.tools.GetOffsetShell \
    --bootstrap-server kafka-1:9092,kafka-2:9093,kafka-3:9094 \
    --topic dump-demo \
    --time -1 | awk -F: '{sum += $3} END {print sum}')
  
  echo "Current messages: $CURRENT"
  echo "Messages per day: $MESSAGES_PER_DAY"
  echo "With 7-day retention: $((MESSAGES_PER_DAY * 7)) messages"
```

---

### 4. Upgrade Compatibility Check
**Scenario:** Upgrade Kafka - over client compatibility:

```terminal:execute
command: |
  echo "=== Pre-Upgrade API Check ==="
  docker exec kafka-1 kafka-broker-api-versions.sh \
    --bootstrap-server kafka-1:9092 | grep -E "Produce|Fetch|ApiVersions"
```

---

## üîç Kafka UI Verification

Otvor Kafka UI:

```dashboard:open-dashboard
name: Kafka UI
url: {{ ingress_protocol }}://{{ session_namespace }}-kafka-ui.{{ ingress_domain }}
```

**ƒåo kontrolova≈•:**
- Topics ‚Üí `delete-demo` ‚Üí Messages tab ‚Üí First offset = 20 (deleted 0-19!)
- Topics ‚Üí `dump-demo` ‚Üí Overview ‚Üí Segment files
- Brokers ‚Üí API versions (if supported by UI)

---

## ‚ö†Ô∏è Common Errors

### 1. "File not found" (dump-log)
```
ERROR Log file not found
```
**Rie≈°enie:**
- Over path: `find /var/lib/kafka/data -name "*.log"`
- Log files s√∫ len na brokerovi, kde je replika

### 2. "Offset out of range" (delete-records)
```
ERROR Offset 100 is out of range
```
**Rie≈°enie:**
- Over latest offset: `kafka-get-offsets.sh --time -1`
- Offset v JSON mus√≠ by≈• <= latest

### 3. "Cannot delete records in future"
```
ERROR Cannot delete future records
```
**Rie≈°enie:**
- Offset v JSON je vy≈°≈°√≠ ne≈æ current latest
- Pou≈æi `--time -1` pre zistenie latest

### 4. "Corrupted log segment"
```
ERROR Corrupted record at offset 123
```
**Rie≈°enie:**
- Use `--deep-iteration` pre full scan
- Mo≈æno treba delete segment a recover z repliky

---

## üéì Best Practices

‚úÖ **DO:**
- **Backup** pred `kafka-delete-records.sh` (permanent!)
- Pou≈æ√≠vaj `dump-log.sh` s `--deep-iteration` pre thorough audit
- Dokumentuj GDPR deletions (audit trail)
- Test delete operations na DEV najprv
- Monitoruj API versions pri upgrade

‚ùå **DON'T:**
- **NEVER** delete records v produkcii bez approval
- Nedump logs poƒças high traffic (disk I/O impact)
- Nezabudni na repliky - delete len na leader nestaƒç√≠
- Nepou≈æ√≠vaj wildcard offsets (must be specific)

---

## üìä Tools Comparison

| Tool | Use Case | Danger Level |
|------|----------|--------------|
| **dump-log** | Debugging, inspection | ‚úÖ Safe (read-only) |
| **get-offsets** | Capacity, monitoring | ‚úÖ Safe (read-only) |
| **delete-records** | GDPR, cleanup | ‚ö†Ô∏è HIGH (permanent delete) |
| **broker-api-versions** | Compatibility | ‚úÖ Safe (read-only) |

---

## üéØ Summary

Nauƒçili sme sa:
- ‚úÖ `kafka-dump-log.sh` - Raw log inspection, corruption detection
- ‚úÖ `kafka-get-offsets.sh` - Offset queries (earliest, latest, timestamp)
- ‚úÖ `kafka-delete-records.sh` - Permanent record deletion (GDPR)
- ‚úÖ `kafka-broker-api-versions.sh` - API compatibility checking
- ‚úÖ Real-world use cases (GDPR, debugging, capacity planning, upgrades)

**Next:** Workshop summary a final review! üöÄ
